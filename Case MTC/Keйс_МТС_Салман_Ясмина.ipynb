{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bc12eae-7c7a-4f40-bcaf-5aacab8f251e",
   "metadata": {},
   "source": [
    "**ДЗ от школы МТС выпонлнила Салман Ясмина Идрис, НИУ ВШЭ, ОП Экономика, 2 курс**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b483b05-4fe3-413b-91d2-43c05a14493b",
   "metadata": {},
   "source": [
    "### Выгрузим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "0a6586a3-29ce-4d60-a322-ef907fb0555f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Two hearts, one valve',\n",
       " \"Pumpin' the blood, we were the flood\",\n",
       " 'We were the body and',\n",
       " 'Two lives, one life',\n",
       " \"Stickin' it out, lettin' you down\",\n",
       " \"Makin' it right\",\n",
       " 'Seasons, they will change',\n",
       " 'Life will make you grow',\n",
       " 'Dreams will make you cry, cry, cry',\n",
       " 'Everything is temporary',\n",
       " 'Everything will slide',\n",
       " 'Love will never die, die, die',\n",
       " 'I know that',\n",
       " 'Ooh, birds fly in different directions',\n",
       " 'Ooh, I hope to see you again',\n",
       " 'Sunsets, sunrises',\n",
       " \"Livin' the dream, watchin' the leaves\",\n",
       " \"Changin' the seasons\",\n",
       " 'Some nights I think of you',\n",
       " \"Relivin' the past, wishin' it'd last\",\n",
       " \"Wishin' and dreamin'\",\n",
       " 'Seasons, they will change',\n",
       " 'Life will make you grow',\n",
       " 'Death can make you hard, hard, hard',\n",
       " 'Everything is temporary',\n",
       " 'Everything will slide',\n",
       " 'Love will never die, die, die',\n",
       " 'I know that',\n",
       " 'Ooh, birds fly in different directions',\n",
       " 'Ooh, I hope to see you again',\n",
       " 'Ooh, birds fly in different directions',\n",
       " 'Ooh, so fly high, so fly high',\n",
       " \"When the moon is lookin' down\",\n",
       " 'Shining light upon your ground',\n",
       " \"I'm flyin' up to let you see\",\n",
       " 'That the shadow cast is me',\n",
       " 'I know that',\n",
       " 'Ooh, birds fly in different directions',\n",
       " 'Ooh, I hope to see you again',\n",
       " 'Ooh, birds fly in different directions',\n",
       " 'Ooh, so fly high, so fly high',\n",
       " 'Ooh, so fly high, so fly high',\n",
       " 'Ooh, so fly high, so fly high']"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('Birds.txt') #файл прикрепляю\n",
    "first_song = []\n",
    "for line in f:\n",
    "    first_song.append(line.strip())\n",
    "first_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "40ae4ee5-4df1-411a-8c73-6d1edd8f6c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I've been lost inside a million eyes\",\n",
       " \"They don't see me, they don't know what it's like\",\n",
       " 'Trading colours into black and white',\n",
       " \"No one's reading all the words that I write\",\n",
       " 'Low, feel the weight of the world in my bones',\n",
       " \"Try to swim but I'm sinking alone\",\n",
       " 'Always falling in the deep unknown',\n",
       " \"Now I'm fighting with my hands up, hands up\",\n",
       " 'Feel the bullets from your head rush, head rush',\n",
       " \"I can see you but I can't touch, can't touch\",\n",
       " \"'Cause I feel numb\",\n",
       " 'So infected with your bad blood, bad blood',\n",
       " 'Keep on running till it blows up, blows up',\n",
       " 'All I wanted was a real love',\n",
       " 'But I feel numb',\n",
       " 'Silent voices to a distant crowd',\n",
       " \"I'm still singing but there's no one around\",\n",
       " 'I keep screaming till my lungs run out',\n",
       " 'But no one listens, no words coming out',\n",
       " 'Low, feel the weight of the world in my bones',\n",
       " \"Try to swim but I'm sinking alone\",\n",
       " 'Always falling in the deep unknown',\n",
       " \"Now I'm fighting with my hands up, hands up\",\n",
       " 'Feel the bullets from your head rush, head rush',\n",
       " \"I can see you but I can't touch, can't touch\",\n",
       " \"'Cause I feel numb\",\n",
       " 'So infected with your bad blood, bad blood',\n",
       " 'Keep on running till it blows up, blows up',\n",
       " 'All I wanted was a real love',\n",
       " 'But I feel numb',\n",
       " 'I feel numb, numb',\n",
       " '(All I wanted was a real love)',\n",
       " 'Numb',\n",
       " '(But I feel numb)',\n",
       " 'I feel numb, numb',\n",
       " '(All I wanted was a real love)',\n",
       " 'Numb',\n",
       " '(But I feel numb)',\n",
       " 'Low, feel the weight of the world in my bones',\n",
       " \"Try to swim but I'm sinking alone\",\n",
       " 'Always falling in the deep unknown',\n",
       " \"Now I'm fighting with my hands up, hands up\",\n",
       " 'Feel the bullets from your head rush, head rush',\n",
       " \"I can see you but I can't touch, can't touch\",\n",
       " \"'Cause I feel numb\",\n",
       " 'So infected with your bad blood, bad blood',\n",
       " 'Keep on running till it blows up, blows up',\n",
       " 'All I wanted was a real love',\n",
       " 'But I feel numb',\n",
       " 'I feel numb, numb',\n",
       " '(All I wanted was a real love)',\n",
       " 'Numb',\n",
       " '(But I feel numb)',\n",
       " 'I feel numb, numb',\n",
       " '(All I wanted was a real love)',\n",
       " '(But I feel numb)']"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('Numb_Dotan.txt')\n",
    "second_song = []\n",
    "for line in f:\n",
    "    second_song.append(line.strip())\n",
    "second_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "626b5fff-aea4-4021-80d2-a3b522dd5d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I've never seen a diamond in the flesh\",\n",
       " 'I cut my teeth on wedding rings in the movies',\n",
       " \"And I'm not proud of my address\",\n",
       " 'In the torn uptown, no post code envy',\n",
       " \"But every song's like\",\n",
       " 'Gold teeth',\n",
       " 'Grey Goose',\n",
       " 'Tripping in the bathroom',\n",
       " 'Bloodstains',\n",
       " 'Ball gowns',\n",
       " 'Trashing the hotel room',\n",
       " \"We don't care, we're driving Cadillacs in our dreams\",\n",
       " \"But everybody's like\",\n",
       " 'Crystal',\n",
       " 'Maybach',\n",
       " 'Diamonds on your timepiece',\n",
       " 'Jet planes',\n",
       " 'Islands',\n",
       " 'Tigers on a gold leash',\n",
       " \"We don't care, we aren't caught up in your love affair\",\n",
       " \"And we'll never be royals\",\n",
       " \"It don't run in our blood\",\n",
       " \"That kind of lux just ain't for us,\",\n",
       " 'We crave a different kind of buzz',\n",
       " 'Let me be your ruler',\n",
       " 'You can call me queen bee',\n",
       " \"And baby I'll rule, I'll rule, I'll rule, I'll rule\",\n",
       " 'Let me live that fantasy',\n",
       " \"My friends and I we've cracked the code\",\n",
       " 'We count our dollars on the train to the party',\n",
       " 'And everyone who knows us knows',\n",
       " \"That we're fine with this, we didn't come from money\",\n",
       " \"But every song's like:\",\n",
       " 'Gold teeth',\n",
       " 'Grey Goose',\n",
       " 'Tripping in the bathroom',\n",
       " 'Bloodstains',\n",
       " 'Ball gowns',\n",
       " 'Trashing the hotel room',\n",
       " \"We don't care, we're driving Cadillacs in our dreams\",\n",
       " \"But everybody's like:\",\n",
       " 'Crystal',\n",
       " 'Maybach',\n",
       " 'Diamonds on your timepiece',\n",
       " 'Jet planes',\n",
       " 'Islands',\n",
       " 'Tigers on a gold leash',\n",
       " \"We don't care, we aren't caught up in your love affair\",\n",
       " \"And we'll never be royals\",\n",
       " \"It don't run in our blood\",\n",
       " \"That kind of lux just ain't for us,\",\n",
       " 'we crave a different kind of buzz',\n",
       " 'Let me be your ruler (ruler)',\n",
       " 'You can call me queen bee',\n",
       " \"And baby I'll rule, I'll rule, I'll rule, I'll rule\",\n",
       " 'Let me live that fantasy',\n",
       " \"We're better than we've ever dreamed\",\n",
       " \"And I'm in love with being queen\",\n",
       " 'Life is great without a care',\n",
       " \"We aren't caught up in your love affair\",\n",
       " \"And we'll never be royals\",\n",
       " \"It don't run in our blood\",\n",
       " \"That kind of lux just ain't for us,\",\n",
       " 'We crave a different kind of buzz',\n",
       " 'Let me be your ruler (ruler)',\n",
       " 'You can call me queen bee',\n",
       " \"And baby I'll rule, I'll rule, I'll rule, I'll rule\",\n",
       " 'Let me live that fantasy']"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('Royals.txt')\n",
    "third_song = []\n",
    "for line in f:\n",
    "    third_song.append(line.strip())\n",
    "third_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "e3b202c3-243d-4c8f-a3e2-f60b0f49417b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Two hearts, one valve',\n",
       " \"Pumpin' the blood, we were the flood\",\n",
       " 'We were the body and',\n",
       " 'Two lives, one life',\n",
       " \"Stickin' it out, lettin' you down\",\n",
       " \"Makin' it right\",\n",
       " 'Seasons, they will change',\n",
       " 'Life will make you grow',\n",
       " 'Dreams will make you cry, cry, cry',\n",
       " 'Everything is temporary',\n",
       " 'Everything will slide',\n",
       " 'Love will never die, die, die',\n",
       " 'I know that',\n",
       " 'Ooh, birds fly in different directions',\n",
       " 'Ooh, I hope to see you again',\n",
       " 'Sunsets, sunrises',\n",
       " \"Livin' the dream, watchin' the leaves\",\n",
       " \"Changin' the seasons\",\n",
       " 'Some nights I think of you',\n",
       " \"Relivin' the past, wishin' it'd last\",\n",
       " \"Wishin' and dreamin'\",\n",
       " 'Seasons, they will change',\n",
       " 'Life will make you grow',\n",
       " 'Death can make you hard, hard, hard',\n",
       " 'Everything is temporary',\n",
       " 'Everything will slide',\n",
       " 'Love will never die, die, die',\n",
       " 'I know that',\n",
       " 'Ooh, birds fly in different directions',\n",
       " 'Ooh, I hope to see you again',\n",
       " 'Ooh, birds fly in different directions',\n",
       " 'Ooh, so fly high, so fly high',\n",
       " \"When the moon is lookin' down\",\n",
       " 'Shining light upon your ground',\n",
       " \"I'm flyin' up to let you see\",\n",
       " 'That the shadow cast is me',\n",
       " 'I know that',\n",
       " 'Ooh, birds fly in different directions',\n",
       " 'Ooh, I hope to see you again',\n",
       " 'Ooh, birds fly in different directions',\n",
       " 'Ooh, so fly high, so fly high',\n",
       " 'Ooh, so fly high, so fly high',\n",
       " 'Ooh, so fly high, so fly high',\n",
       " \"I've been lost inside a million eyes\",\n",
       " \"They don't see me, they don't know what it's like\",\n",
       " 'Trading colours into black and white',\n",
       " \"No one's reading all the words that I write\",\n",
       " 'Low, feel the weight of the world in my bones',\n",
       " \"Try to swim but I'm sinking alone\",\n",
       " 'Always falling in the deep unknown',\n",
       " \"Now I'm fighting with my hands up, hands up\",\n",
       " 'Feel the bullets from your head rush, head rush',\n",
       " \"I can see you but I can't touch, can't touch\",\n",
       " \"'Cause I feel numb\",\n",
       " 'So infected with your bad blood, bad blood',\n",
       " 'Keep on running till it blows up, blows up',\n",
       " 'All I wanted was a real love',\n",
       " 'But I feel numb',\n",
       " 'Silent voices to a distant crowd',\n",
       " \"I'm still singing but there's no one around\",\n",
       " 'I keep screaming till my lungs run out',\n",
       " 'But no one listens, no words coming out',\n",
       " 'Low, feel the weight of the world in my bones',\n",
       " \"Try to swim but I'm sinking alone\",\n",
       " 'Always falling in the deep unknown',\n",
       " \"Now I'm fighting with my hands up, hands up\",\n",
       " 'Feel the bullets from your head rush, head rush',\n",
       " \"I can see you but I can't touch, can't touch\",\n",
       " \"'Cause I feel numb\",\n",
       " 'So infected with your bad blood, bad blood',\n",
       " 'Keep on running till it blows up, blows up',\n",
       " 'All I wanted was a real love',\n",
       " 'But I feel numb',\n",
       " 'I feel numb, numb',\n",
       " '(All I wanted was a real love)',\n",
       " 'Numb',\n",
       " '(But I feel numb)',\n",
       " 'I feel numb, numb',\n",
       " '(All I wanted was a real love)',\n",
       " 'Numb',\n",
       " '(But I feel numb)',\n",
       " 'Low, feel the weight of the world in my bones',\n",
       " \"Try to swim but I'm sinking alone\",\n",
       " 'Always falling in the deep unknown',\n",
       " \"Now I'm fighting with my hands up, hands up\",\n",
       " 'Feel the bullets from your head rush, head rush',\n",
       " \"I can see you but I can't touch, can't touch\",\n",
       " \"'Cause I feel numb\",\n",
       " 'So infected with your bad blood, bad blood',\n",
       " 'Keep on running till it blows up, blows up',\n",
       " 'All I wanted was a real love',\n",
       " 'But I feel numb',\n",
       " 'I feel numb, numb',\n",
       " '(All I wanted was a real love)',\n",
       " 'Numb',\n",
       " '(But I feel numb)',\n",
       " 'I feel numb, numb',\n",
       " '(All I wanted was a real love)',\n",
       " '(But I feel numb)',\n",
       " \"I've never seen a diamond in the flesh\",\n",
       " 'I cut my teeth on wedding rings in the movies',\n",
       " \"And I'm not proud of my address\",\n",
       " 'In the torn uptown, no post code envy',\n",
       " \"But every song's like\",\n",
       " 'Gold teeth',\n",
       " 'Grey Goose',\n",
       " 'Tripping in the bathroom',\n",
       " 'Bloodstains',\n",
       " 'Ball gowns',\n",
       " 'Trashing the hotel room',\n",
       " \"We don't care, we're driving Cadillacs in our dreams\",\n",
       " \"But everybody's like\",\n",
       " 'Crystal',\n",
       " 'Maybach',\n",
       " 'Diamonds on your timepiece',\n",
       " 'Jet planes',\n",
       " 'Islands',\n",
       " 'Tigers on a gold leash',\n",
       " \"We don't care, we aren't caught up in your love affair\",\n",
       " \"And we'll never be royals\",\n",
       " \"It don't run in our blood\",\n",
       " \"That kind of lux just ain't for us,\",\n",
       " 'We crave a different kind of buzz',\n",
       " 'Let me be your ruler',\n",
       " 'You can call me queen bee',\n",
       " \"And baby I'll rule, I'll rule, I'll rule, I'll rule\",\n",
       " 'Let me live that fantasy',\n",
       " \"My friends and I we've cracked the code\",\n",
       " 'We count our dollars on the train to the party',\n",
       " 'And everyone who knows us knows',\n",
       " \"That we're fine with this, we didn't come from money\",\n",
       " \"But every song's like:\",\n",
       " 'Gold teeth',\n",
       " 'Grey Goose',\n",
       " 'Tripping in the bathroom',\n",
       " 'Bloodstains',\n",
       " 'Ball gowns',\n",
       " 'Trashing the hotel room',\n",
       " \"We don't care, we're driving Cadillacs in our dreams\",\n",
       " \"But everybody's like:\",\n",
       " 'Crystal',\n",
       " 'Maybach',\n",
       " 'Diamonds on your timepiece',\n",
       " 'Jet planes',\n",
       " 'Islands',\n",
       " 'Tigers on a gold leash',\n",
       " \"We don't care, we aren't caught up in your love affair\",\n",
       " \"And we'll never be royals\",\n",
       " \"It don't run in our blood\",\n",
       " \"That kind of lux just ain't for us,\",\n",
       " 'we crave a different kind of buzz',\n",
       " 'Let me be your ruler (ruler)',\n",
       " 'You can call me queen bee',\n",
       " \"And baby I'll rule, I'll rule, I'll rule, I'll rule\",\n",
       " 'Let me live that fantasy',\n",
       " \"We're better than we've ever dreamed\",\n",
       " \"And I'm in love with being queen\",\n",
       " 'Life is great without a care',\n",
       " \"We aren't caught up in your love affair\",\n",
       " \"And we'll never be royals\",\n",
       " \"It don't run in our blood\",\n",
       " \"That kind of lux just ain't for us,\",\n",
       " 'We crave a different kind of buzz',\n",
       " 'Let me be your ruler (ruler)',\n",
       " 'You can call me queen bee',\n",
       " \"And baby I'll rule, I'll rule, I'll rule, I'll rule\",\n",
       " 'Let me live that fantasy']"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cоединим тексты песен\n",
    "\n",
    "first_song.extend(second_song)\n",
    "first_song.extend(third_song)\n",
    "songs = first_song\n",
    "songs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d43cea8-efb1-48c5-b796-eb08837da2e9",
   "metadata": {},
   "source": [
    "### Токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "092cacd2-f1ff-4262-bba3-274c45c62003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "e052b552-ea40-411b-862d-50f9a4624bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\", quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3355ad-c9fb-4cc1-9727-1349bbbf952f",
   "metadata": {},
   "source": [
    "Давайте сначала сделаем самую простую токинизацию - деления текстов по пробельным символам и приведение токенов в нижний регистр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "443674d2-6190-48a0-95c0-b360cc9b18d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cry', 'inside', 'my', 'was', 'our', \"it'd\", 'care', 'but', 'alone', 'buzz', \"watchin'\", 'life', 'islands', 'not', 'weight', 'out', 'dream', 'just', 'cracked', 'different', 'been', 'falling', 'reading', 'from', 'proud']\n"
     ]
    }
   ],
   "source": [
    "dictionary = set() #для сохранения уникальных токенов\n",
    "all_w = [] #для хранения всех токенов, чтобы проверить, как часто они встречаются\n",
    "\n",
    "for line in songs:\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        word = word.replace(',', '')\n",
    "        word = word.replace('(', '')\n",
    "        word = word.replace(')', '')\n",
    "        dictionary.add(word.lower())\n",
    "        all_w.append(word.lower())\n",
    "\n",
    "d = list(dictionary)\n",
    "print(d[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "21ca9576-7287-46b9-a05d-3d6a026c0bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d) #уникальные токены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "a024b3aa-a830-4803-8ade-ecc987cac97f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "961"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_w) #все токены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "2eeb05ee-41f0-444d-b963-ffbaeae74e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 38),\n",
       " ('the', 32),\n",
       " ('in', 25),\n",
       " ('numb', 21),\n",
       " ('feel', 20),\n",
       " ('but', 19),\n",
       " ('you', 16),\n",
       " ('up', 16),\n",
       " ('a', 16),\n",
       " ('your', 15)]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Посмотрим самые популярные токены\n",
    "\n",
    "from collections import Counter\n",
    "counts = Counter(all_w)\n",
    "counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201aaeb6-f6cf-401b-a885-db6624bf32a1",
   "metadata": {},
   "source": [
    "Заметим, что по большей части перед нами стоп-слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "1d6c6347-a689-4aa7-9b07-f46d079a900a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#А теперь удалим стоп-слова\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "dfc9f2e0-c667-4ab8-a3ce-023e2f6df8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199\n",
      "564\n"
     ]
    }
   ],
   "source": [
    "all_w_n = np.array(all_w) #сделаем нампи эррей из листа со всеми токенами\n",
    "\n",
    "mask = np.in1d(all_w_n, stop_words) #создаем маску, которая ищет стоп слова\n",
    "\n",
    "all_w_new = list(all_w_n[~mask]) #оставляем только не стоп-слова \n",
    "\n",
    "print(len(set(all_w_new))) #как мы видим удалость удалить довольно много токенов\n",
    "print(len(all_w_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "49e782b0-e008-4b54-9f62-25105db0ef24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токен: numb, частота: 21\n",
      "Токен: feel, частота: 20\n",
      "Токен: fly, частота: 13\n",
      "Токен: love, частота: 13\n",
      "Токен: rule, частота: 12\n",
      "Токен: i'll, частота: 12\n",
      "Токен: ooh, частота: 12\n",
      "Токен: i'm, частота: 10\n",
      "Токен: blood, частота: 10\n",
      "Токен: high, частота: 8\n"
     ]
    }
   ],
   "source": [
    "# Снова найдем топ-10 слов\n",
    "\n",
    "unique, counts = np.unique(all_w_new, return_counts=True)\n",
    "\n",
    "sorted_ind_asc = np.argsort(counts)\n",
    "sorted_ind_desc = np.argsort(counts)[::-1]\n",
    "\n",
    "for i, j in zip(unique[sorted_ind_desc][:10], counts[sorted_ind_desc][:10]):\n",
    "    print(f'Токен: {i}, частота: {j}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bfc349-97c0-4314-ad5a-0a30f76513ab",
   "metadata": {},
   "source": [
    "Другое дело! Но странно, что i'm не относится к стоп-словам... Давайте теперь посмотрим самые непопулярные токены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "49365717-8070-436d-b084-692145af242c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токен: write, частота: 1\n",
      "Токен: flyin', частота: 1\n",
      "Токен: proud, частота: 1\n",
      "Токен: flood, частота: 1\n",
      "Токен: flesh, частота: 1\n",
      "Токен: fine, частота: 1\n",
      "Токен: pumpin', частота: 1\n",
      "Токен: reading, частота: 1\n",
      "Токен: relivin', частота: 1\n",
      "Токен: right, частота: 1\n"
     ]
    }
   ],
   "source": [
    "for i, j in zip(unique[sorted_ind_asc][:10], counts[sorted_ind_asc][:10]):\n",
    "    print(f'Токен: {i}, частота: {j}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "9cc63509-fa83-4cdc-813c-d04a844c95e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1134\n",
      "['Two', 'hearts', ',', 'one', 'valve', 'Pumpin', \"'\", 'the', 'blood', ',']\n"
     ]
    }
   ],
   "source": [
    "#Тепрь создадим словарь используя токенайзер. Подгрузим предварительно пунктуацию\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "noise = stop_words + list(punctuation) #создадим список со стоп-словами и пунктуацией\n",
    "\n",
    "new_d = []\n",
    "for line in songs:\n",
    "    words = word_tokenize(line)\n",
    "    new_d.extend(words)\n",
    "\n",
    "print(len(new_d)) #чекним сколько у нас токенов (в том числе повторяющихся)\n",
    "print(new_d[:10]) #ну и пару слов выведем "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "af23c1f0-c2d3-4147-ba0b-bd40d16bd6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(new_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e044bd27-2201-4c32-8329-76600707f8f5",
   "metadata": {},
   "source": [
    "Как мы можем заметить их стало больше, но давайте удалим шум, и снова проверим количество токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "5e46c840-affd-498f-8839-635f94492c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "715"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_d_np = np.array(new_d)\n",
    "\n",
    "mask = np.in1d(new_d_np, noise) #создаем маску, которая ищет шум\n",
    "\n",
    "new_d_np_cleaned = new_d_np[~mask]\n",
    "\n",
    "len(new_d_np_cleaned) #как мы видим удалость удалить довольно много токенов, но их все равно больше, чем в \"ручном\" токенайзере"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "ceef6e47-c76a-4bed-bc3b-80614537d984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токен: I, частота: 62\n",
      "Токен: n't, частота: 22\n",
      "Токен: numb, частота: 18\n",
      "Токен: feel, частота: 17\n",
      "Токен: 'll, частота: 15\n",
      "Токен: fly, частота: 13\n",
      "Токен: But, частота: 12\n",
      "Токен: Ooh, частота: 12\n",
      "Токен: rule, частота: 12\n",
      "Токен: love, частота: 11\n"
     ]
    }
   ],
   "source": [
    "#снова проверим топ-10 популярных токенов\n",
    "\n",
    "unique, counts = np.unique(new_d_np_cleaned, return_counts=True)\n",
    "\n",
    "sorted_ind_asc = np.argsort(counts)\n",
    "sorted_ind_desc = np.argsort(counts)[::-1]\n",
    "\n",
    "for i, j in zip(unique[sorted_ind_desc][:10], counts[sorted_ind_desc][:10]):\n",
    "    print(f'Токен: {i}, частота: {j}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d6f082-7f1d-47d5-b89c-d6028c1b7a83",
   "metadata": {},
   "source": [
    "Как-то мне не очень нравится, как все сработало. Давайте воспользуемся другим токенайзером. А точнее тем, который используюь для анализа твитов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "a7d20c3a-53cb-4d1a-ae0f-1c8c6675f5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1074\n",
      "['Two', 'hearts', ',', 'one', 'valve', 'Pumpin', \"'\", 'the', 'blood', ',']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tw = TweetTokenizer() #создаем объект класса TweetTokenizer\n",
    "new_d = []\n",
    "for line in songs:\n",
    "    new_d.extend(tw.tokenize(line))\n",
    "    \n",
    "print(len(new_d)) #чекним сколько у нас токенов\n",
    "print(new_d[:10]) #ну и пару слов выведем "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "9e86dae4-c53d-4d92-aee7-02f4f6c7f6fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "662"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#удалим шум\n",
    "\n",
    "new_d_np = np.array(new_d)\n",
    "\n",
    "mask = np.in1d(new_d_np, noise)\n",
    "\n",
    "new_d_np_cleaned = new_d_np[~mask]\n",
    "\n",
    "len(new_d_np_cleaned) #чуть лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "0f7f9ea5-e5e4-4314-b778-f4547f74e133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токен: I, частота: 38\n",
      "Токен: numb, частота: 18\n",
      "Токен: feel, частота: 17\n",
      "Токен: fly, частота: 13\n",
      "Токен: Ooh, частота: 12\n",
      "Токен: I'll, частота: 12\n",
      "Токен: rule, частота: 12\n",
      "Токен: But, частота: 12\n",
      "Токен: love, частота: 11\n",
      "Токен: I'm, частота: 10\n"
     ]
    }
   ],
   "source": [
    "#снова проверим топ-10 популярных токенов\n",
    "\n",
    "unique, counts = np.unique(new_d_np_cleaned, return_counts=True)\n",
    "\n",
    "sorted_ind_asc = np.argsort(counts)\n",
    "sorted_ind_desc = np.argsort(counts)[::-1]\n",
    "\n",
    "for i, j in zip(unique[sorted_ind_desc][:10], counts[sorted_ind_desc][:10]):\n",
    "    print(f'Токен: {i}, частота: {j}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcb80a2-7eda-4609-85a7-1d0aee2f74b3",
   "metadata": {},
   "source": [
    "Тоже не очень, поскольку не переводит слова в нижний регистр, да и i'm не удаляет. Вывод: песни довольно чистый наборы текста, поэтому нам не нужны какие-то уникальные токенайзеры, наш собственный работает лучше - так что оформим его в отдельную функцию и будем далее использовать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "ddf1842c-7907-4292-9141-edfa779c715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(text):\n",
    "    words = text.split()\n",
    "    all_w = []\n",
    "    for word in words:\n",
    "        word = word.replace(',', '')\n",
    "        word = word.replace('(', '')\n",
    "        word = word.replace(')', '')\n",
    "        all_w.append(word.lower())\n",
    "    dict_cleaned = []\n",
    "    for token in all_w:\n",
    "        if token not in noise and token != \"i'm\":\n",
    "            dict_cleaned.append(token)\n",
    "    return dict_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbdf46b-aa3a-4bb8-a823-d6a83ef0d966",
   "metadata": {},
   "source": [
    "### Стемминг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "d2d413bd-fe03-4ae0-9a98-2c968f8bf396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#давайте добавим стемминг в наш токенайзер\n",
    "\n",
    "from nltk import SnowballStemmer\n",
    "\n",
    "def custom_stem_tokenizer(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    dictionary_cleaned = custom_tokenizer(text)\n",
    "    stemmed = [stemmer.stem(w) for w in dictionary_cleaned]\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "c5b29527-5f3a-4714-b030-e39b5e1ecba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "554"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_new = []\n",
    "for line in songs:\n",
    "    words = custom_stem_tokenizer(line)\n",
    "    dict_new.extend(words)\n",
    "len(dict_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "ce788c36-0507-4211-88af-58d13fad14b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(dict_new)) #как мы видим, совсем немного токенов осталось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "8cfc8536-cd43-46b3-8d8f-ad1d5bdbf21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('numb', 21),\n",
       " ('feel', 20),\n",
       " ('love', 13),\n",
       " ('fli', 13),\n",
       " ('ooh', 12),\n",
       " (\"i'll\", 12),\n",
       " ('rule', 12),\n",
       " ('blood', 10),\n",
       " ('differ', 8),\n",
       " ('see', 8)]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#найдем самые популярные\n",
    "from collections import Counter\n",
    "counts = Counter(dict_new)\n",
    "counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40004c2-583b-44bb-a786-1d39cf987a1c",
   "metadata": {},
   "source": [
    "### Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "f990b13d-f830-49c3-a199-f1a17c157707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "f846ad4e-6a7c-4791-8afc-a0c6d41b3a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Было: Now I'm fighting with my hands up, hands up\n",
      "Cтало: ['fighting', 'hand', 'hand']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "text = songs[50]\n",
    "dict_n = custom_tokenizer(text)\n",
    "lemmatized = [lemmatizer.lemmatize(w) for w in dict_n]\n",
    "print(f'Было: {text}')\n",
    "print(f'Cтало: {lemmatized}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "2474fb47-f050-4c3c-86ee-123060455a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Добавим вместо стемминга лемматизацию в наш токнезатор\n",
    "\n",
    "def custom_stem_tokenizer(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    dictionary_cleaned = custom_tokenizer(text)\n",
    "    lemmatized = [lemmatizer.lemmatize(w) for w in dictionary_cleaned]\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "5bad3ae0-c7ca-4a11-a303-bb9c4557858a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "554"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_new = []\n",
    "for line in songs:\n",
    "    words = custom_stem_tokenizer(line)\n",
    "    dict_new.extend(words)\n",
    "len(dict_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "058da2e8-144c-4ef8-89b6-158fd930f622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(dict_new)) #чуть больше уникальных токенов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b990e1-7c3f-4ff8-a005-cc945015aaa6",
   "metadata": {},
   "source": [
    "### POS-тэггинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "fac1bebc-16ab-4528-8088-3718c01ed68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "ec8bc908-2f2c-4cdc-a640-4459cd1daaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "c10081bb-15c1-49ea-b669-e1ae1f9e4f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cry', 'NN'),\n",
       " ('inside', 'IN'),\n",
       " ('plane', 'NN'),\n",
       " (\"it'd\", 'NN'),\n",
       " ('care', 'NN'),\n",
       " ('alone', 'RB'),\n",
       " ('tiger', 'RB'),\n",
       " ('buzz', 'VB'),\n",
       " (\"watchin'\", 'JJ'),\n",
       " ('life', 'NN'),\n",
       " ('direction', 'NN'),\n",
       " ('weight', 'VBD'),\n",
       " ('dream', 'NN'),\n",
       " ('cracked', 'VBN'),\n",
       " ('different', 'JJ'),\n",
       " ('proud', 'JJ'),\n",
       " ('leaf', 'NN'),\n",
       " ('reading', 'VBG'),\n",
       " ('falling', 'VBG'),\n",
       " ('bathroom', 'NN'),\n",
       " ('bone', 'NN'),\n",
       " ('still', 'RB'),\n",
       " ('cadillacs', 'VB'),\n",
       " ('past', 'JJ'),\n",
       " ('live', 'NN'),\n",
       " ('without', 'IN'),\n",
       " ('uptown', 'JJ'),\n",
       " ('sunrise', 'NN'),\n",
       " ('shining', 'VBG'),\n",
       " ('love', 'NN'),\n",
       " ('count', 'NN'),\n",
       " ('word', 'NN'),\n",
       " ('million', 'CD'),\n",
       " ('hand', 'NN'),\n",
       " ('maybach', 'NN'),\n",
       " ('valve', 'IN'),\n",
       " ('colour', 'NN'),\n",
       " ('affair', 'NN'),\n",
       " ('fighting', 'VBG'),\n",
       " ('ooh', 'JJ'),\n",
       " (\"flyin'\", 'JJ'),\n",
       " ('money', 'NN'),\n",
       " ('kind', 'NN'),\n",
       " ('gown', 'JJ'),\n",
       " ('come', 'JJ'),\n",
       " ('silent', 'NN'),\n",
       " ('high', 'JJ'),\n",
       " ('head', 'NN'),\n",
       " ('better', 'RBR'),\n",
       " (\"makin'\", 'NN'),\n",
       " (\"wishin'\", 'NN'),\n",
       " ('feel', 'VBP'),\n",
       " ('upon', 'IN'),\n",
       " ('see', 'VBP'),\n",
       " ('wedding', 'VBG'),\n",
       " ('black', 'JJ'),\n",
       " ('gold', 'NN'),\n",
       " ('ring', 'VBG'),\n",
       " ('touch', 'JJ'),\n",
       " ('two', 'CD'),\n",
       " ('bullet', 'NN'),\n",
       " ('let', 'VB'),\n",
       " (\"one's\", 'PRP'),\n",
       " (\"we're\", 'VB'),\n",
       " ('ruler', 'NN'),\n",
       " ('singing', 'VBG'),\n",
       " ('keep', 'VB'),\n",
       " (\"livin'\", 'NN'),\n",
       " ('baby', 'NN'),\n",
       " ('like:', 'VBD'),\n",
       " (\"changin'\", 'NN'),\n",
       " ('running', 'VBG'),\n",
       " (\"i'll\", 'JJ'),\n",
       " ('caught', 'VBN'),\n",
       " ('white', 'JJ'),\n",
       " ('sinking', 'NN'),\n",
       " (\"lettin'\", 'NN'),\n",
       " ('trading', 'NN'),\n",
       " ('room', 'NN'),\n",
       " ('bad', 'JJ'),\n",
       " ('heart', 'NN'),\n",
       " ('real', 'JJ'),\n",
       " ('post', 'NN'),\n",
       " (\"ain't\", 'NN'),\n",
       " ('flood', 'VBD'),\n",
       " ('deep', 'JJ'),\n",
       " ('grey', 'NN'),\n",
       " ('never', 'RB'),\n",
       " (\"there's\", 'VBD'),\n",
       " ('swim', 'NN'),\n",
       " ('crowd', 'NN'),\n",
       " ('lung', 'NN'),\n",
       " ('fly', 'VBP'),\n",
       " ('numb', 'JJ'),\n",
       " ('teeth', 'NNS'),\n",
       " ('coming', 'VBG'),\n",
       " ('trashing', 'VBG'),\n",
       " ('diamond', 'NN'),\n",
       " ('crave', 'VBP'),\n",
       " ('fine', 'JJ'),\n",
       " ('shadow', 'NN'),\n",
       " ('royal', 'NN'),\n",
       " ('bird', 'NN'),\n",
       " ('sunset', 'VBN'),\n",
       " ('death', 'NN'),\n",
       " ('wanted', 'VBD'),\n",
       " ('slide', 'NN'),\n",
       " ('u', 'JJ'),\n",
       " (\"song's\", 'NN'),\n",
       " ('torn', 'JJ'),\n",
       " ('distant', 'JJ'),\n",
       " ('lux', 'NN'),\n",
       " (\"i've\", 'NN'),\n",
       " ('light', 'NN'),\n",
       " ('every', 'DT'),\n",
       " ('eye', 'NN'),\n",
       " ('bee', 'NN'),\n",
       " ('bloodstain', 'VBP'),\n",
       " ('dollar', 'NN'),\n",
       " ('cut', 'NN'),\n",
       " ('make', 'VBP'),\n",
       " ('listens', 'NNS'),\n",
       " ('till', 'VB'),\n",
       " (\"lookin'\", 'JJ'),\n",
       " ('timepiece', 'NN'),\n",
       " ('everything', 'NN'),\n",
       " ('always', 'RB'),\n",
       " ('fantasy', 'RB'),\n",
       " ('know', 'JJ'),\n",
       " ('envy', 'NN'),\n",
       " ('friend', 'NN'),\n",
       " ('world', 'NN'),\n",
       " ('infected', 'VBN'),\n",
       " (\"'cause\", 'CD'),\n",
       " ('train', 'NN'),\n",
       " ('die', 'NN'),\n",
       " ('ground', 'NN'),\n",
       " ('write', 'IN'),\n",
       " ('screaming', 'VBG'),\n",
       " ('hotel', 'NN'),\n",
       " ('change', 'NN'),\n",
       " ('driving', 'VBG'),\n",
       " (\"we'll\", 'NN'),\n",
       " ('call', 'NN'),\n",
       " ('rush', 'NN'),\n",
       " ('everyone', 'NN'),\n",
       " ('think', 'VBP'),\n",
       " (\"dreamin'\", 'NN'),\n",
       " ('around', 'IN'),\n",
       " (\"we've\", 'NN'),\n",
       " ('last', 'JJ'),\n",
       " ('dreamed', 'JJ'),\n",
       " ('rule', 'NN'),\n",
       " ('jet', 'NN'),\n",
       " ('hard', 'RB'),\n",
       " (\"stickin'\", 'VBZ'),\n",
       " ('moon', 'NN'),\n",
       " ('hope', 'NN'),\n",
       " ('night', 'NN'),\n",
       " ('try', 'VB'),\n",
       " ('voice', 'NN'),\n",
       " ('temporary', 'JJ'),\n",
       " ('address', 'NN'),\n",
       " (\"pumpin'\", 'NN'),\n",
       " ('tripping', 'VBG'),\n",
       " ('queen', 'JJ'),\n",
       " (\"everybody's\", 'JJ'),\n",
       " ('island', 'NN'),\n",
       " ('great', 'JJ'),\n",
       " ('flesh', 'JJ'),\n",
       " ('run', 'NN'),\n",
       " ('grow', 'NN'),\n",
       " ('body', 'NN'),\n",
       " ('seen', 'VBN'),\n",
       " ('movie', 'NN'),\n",
       " ('like', 'IN'),\n",
       " ('right', 'JJ'),\n",
       " ('goose', 'NN'),\n",
       " ('blood', 'NN'),\n",
       " ('leash', 'NN'),\n",
       " ('one', 'CD'),\n",
       " ('unknown', 'JJ'),\n",
       " ('party', 'NN'),\n",
       " ('lost', 'VBD'),\n",
       " ('code', 'NN'),\n",
       " (\"relivin'\", 'NN'),\n",
       " ('ever', 'RB'),\n",
       " ('crystal', 'JJ'),\n",
       " ('season', 'NN'),\n",
       " (\"can't\", 'NN'),\n",
       " ('ball', 'VB'),\n",
       " ('blow', 'IN'),\n",
       " ('cast', 'NN'),\n",
       " ('low', 'JJ')]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(set(dict_new)) # тэггинг"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1b5f55-c18a-4c25-ba7f-9ea493769193",
   "metadata": {},
   "source": [
    "### Тематическое моделирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "962c7991-3ec3-4619-9af7-a46f9ce2e085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тoпик 1: 0.033*\"fly\" + 0.028*\"blows\" + 0.028*\"high\" + 0.020*\"till\" + 0.020*\"keep\" + 0.019*\"ooh\" + 0.015*\"running\" + 0.015*\"make\" + 0.015*\"life\" + 0.015*\"cry\"\n",
      "Тoпик 2: 0.053*\"numb\" + 0.041*\"feel\" + 0.035*\"love\" + 0.025*\"real\" + 0.025*\"different\" + 0.024*\"wanted\" + 0.022*\"die\" + 0.021*\"rule\" + 0.021*\"never\" + 0.017*\"ooh\"\n",
      "Тoпик 3: 0.032*\"see\" + 0.032*\"i'll\" + 0.031*\"feel\" + 0.029*\"blood\" + 0.026*\"rule\" + 0.024*\"can't\" + 0.024*\"touch\" + 0.024*\"bad\" + 0.024*\"hands\" + 0.022*\"numb\"\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "np.random.seed(25)\n",
    "\n",
    "texts = [custom_tokenizer(line) for line in songs]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "lda_model = LdaModel(corpus, num_topics=3, id2word=dictionary)\n",
    "\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Тoпик {idx + 1}: {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1960d0a-86c2-4133-ada8-6c70b63b525e",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "4332c0ee-009a-4d44-be1b-4600337c0d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Замутим векторизирвание\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(tokenizer=custom_tokenizer)\n",
    "X = cv.fit_transform(songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "a1d03267-7335-498c-ae3d-82c21e35695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(tokenizer=custom_tokenizer)\n",
    "X = tfidf_vec.fit_transform(songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "7e58764e-fbd5-4c8e-9790-08225cb9fce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray() #каждой строчке соответсвует свой список с весами по словам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "90bd63be-3d3a-48f5-9bf3-e4e668e7d48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Now I'm fighting with my hands up, hands up\""
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Возьмем какую-то строчку и оценим важность токенов для нее\n",
    "songs[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "117cd0e2-59cc-4e64-b227-f4573b697147",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = X[50].toarray()         #выгрузим \"веса\" каждого слово для интересующего нас строчки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "904d93bb-a274-450d-8ce1-9deb3cb109a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([65, 79], dtype=int64)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = np.where(importance != 0)[1] #найдем индексы слов с ненулевым весом для нашей строки\n",
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "a747987c-f622-4a38-a1cf-8fc99b822a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fighting': 0.4472135954999579, 'hands': 0.8944271909999159}"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#То\n",
    "imp = {}\n",
    "for i, j in zip(list(importance[:, ind][0]), list(cv.get_feature_names_out()[ind])):\n",
    "    imp[j] = i\n",
    "imp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b5f0cc-803f-4726-9c21-8a36527516cc",
   "metadata": {},
   "source": [
    "То есть относиттельно остальных строк в тексте, здесь довольно часто встречается слово hands и fighting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7396cad-a1c3-4c81-94e4-8f3a1f1fd395",
   "metadata": {},
   "source": [
    "### n-граммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "9562268c-31bb-44b5-b834-e9a74fc2ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "8b4cf70c-096a-4043-a93f-79d02a7ce02a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('everything',), ('slide',)]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = custom_tokenizer(songs[25])\n",
    "list(ngrams(sent, 1))  # униграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "727e0ef6-4831-46db-ac0d-907a1dbb862a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('everything', 'slide')]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sent, 2))  # биграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "0e60983c-6f98-49de-8720-f188236472ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('two', 'heart')\n",
      "('heart', 'one')\n",
      "('one', 'valve')\n",
      "('valve', \"pumpin'\")\n",
      "(\"pumpin'\", 'blood')\n",
      "('blood', 'flood')\n",
      "('flood', 'body')\n",
      "('body', 'two')\n",
      "('two', 'life')\n",
      "('life', 'one')\n",
      "('one', 'life')\n",
      "('life', \"stickin'\")\n",
      "(\"stickin'\", \"lettin'\")\n",
      "(\"lettin'\", \"makin'\")\n",
      "(\"makin'\", 'right')\n",
      "('right', 'season')\n",
      "('season', 'change')\n",
      "('change', 'life')\n",
      "('life', 'make')\n",
      "('make', 'grow')\n",
      "('grow', 'dream')\n",
      "('dream', 'make')\n",
      "('make', 'cry')\n",
      "('cry', 'cry')\n",
      "('cry', 'cry')\n",
      "('cry', 'everything')\n",
      "('everything', 'temporary')\n",
      "('temporary', 'everything')\n",
      "('everything', 'slide')\n",
      "('slide', 'love')\n",
      "('love', 'never')\n",
      "('never', 'die')\n",
      "('die', 'die')\n",
      "('die', 'die')\n",
      "('die', 'know')\n",
      "('know', 'ooh')\n",
      "('ooh', 'bird')\n",
      "('bird', 'fly')\n",
      "('fly', 'different')\n",
      "('different', 'direction')\n",
      "('direction', 'ooh')\n",
      "('ooh', 'hope')\n",
      "('hope', 'see')\n",
      "('see', 'sunset')\n",
      "('sunset', 'sunrise')\n",
      "('sunrise', \"livin'\")\n",
      "(\"livin'\", 'dream')\n",
      "('dream', \"watchin'\")\n",
      "(\"watchin'\", 'leaf')\n",
      "('leaf', \"changin'\")\n",
      "(\"changin'\", 'season')\n",
      "('season', 'night')\n",
      "('night', 'think')\n",
      "('think', \"relivin'\")\n",
      "(\"relivin'\", 'past')\n",
      "('past', \"wishin'\")\n",
      "(\"wishin'\", \"it'd\")\n",
      "(\"it'd\", 'last')\n",
      "('last', \"wishin'\")\n",
      "(\"wishin'\", \"dreamin'\")\n",
      "(\"dreamin'\", 'season')\n",
      "('season', 'change')\n",
      "('change', 'life')\n",
      "('life', 'make')\n",
      "('make', 'grow')\n",
      "('grow', 'death')\n",
      "('death', 'make')\n",
      "('make', 'hard')\n",
      "('hard', 'hard')\n",
      "('hard', 'hard')\n",
      "('hard', 'everything')\n",
      "('everything', 'temporary')\n",
      "('temporary', 'everything')\n",
      "('everything', 'slide')\n",
      "('slide', 'love')\n",
      "('love', 'never')\n",
      "('never', 'die')\n",
      "('die', 'die')\n",
      "('die', 'die')\n",
      "('die', 'know')\n",
      "('know', 'ooh')\n",
      "('ooh', 'bird')\n",
      "('bird', 'fly')\n",
      "('fly', 'different')\n",
      "('different', 'direction')\n",
      "('direction', 'ooh')\n",
      "('ooh', 'hope')\n",
      "('hope', 'see')\n",
      "('see', 'ooh')\n",
      "('ooh', 'bird')\n",
      "('bird', 'fly')\n",
      "('fly', 'different')\n",
      "('different', 'direction')\n",
      "('direction', 'ooh')\n",
      "('ooh', 'fly')\n",
      "('fly', 'high')\n",
      "('high', 'fly')\n",
      "('fly', 'high')\n",
      "('high', 'moon')\n",
      "('moon', \"lookin'\")\n",
      "(\"lookin'\", 'shining')\n",
      "('shining', 'light')\n",
      "('light', 'upon')\n",
      "('upon', 'ground')\n",
      "('ground', \"flyin'\")\n",
      "(\"flyin'\", 'let')\n",
      "('let', 'see')\n",
      "('see', 'shadow')\n",
      "('shadow', 'cast')\n",
      "('cast', 'know')\n",
      "('know', 'ooh')\n",
      "('ooh', 'bird')\n",
      "('bird', 'fly')\n",
      "('fly', 'different')\n",
      "('different', 'direction')\n",
      "('direction', 'ooh')\n",
      "('ooh', 'hope')\n",
      "('hope', 'see')\n",
      "('see', 'ooh')\n",
      "('ooh', 'bird')\n",
      "('bird', 'fly')\n",
      "('fly', 'different')\n",
      "('different', 'direction')\n",
      "('direction', 'ooh')\n",
      "('ooh', 'fly')\n",
      "('fly', 'high')\n",
      "('high', 'fly')\n",
      "('fly', 'high')\n",
      "('high', 'ooh')\n",
      "('ooh', 'fly')\n",
      "('fly', 'high')\n",
      "('high', 'fly')\n",
      "('fly', 'high')\n",
      "('high', 'ooh')\n",
      "('ooh', 'fly')\n",
      "('fly', 'high')\n",
      "('high', 'fly')\n",
      "('fly', 'high')\n",
      "('high', \"i've\")\n",
      "(\"i've\", 'lost')\n",
      "('lost', 'inside')\n",
      "('inside', 'million')\n",
      "('million', 'eye')\n",
      "('eye', 'see')\n",
      "('see', 'know')\n",
      "('know', 'like')\n",
      "('like', 'trading')\n",
      "('trading', 'colour')\n",
      "('colour', 'black')\n",
      "('black', 'white')\n",
      "('white', \"one's\")\n",
      "(\"one's\", 'reading')\n",
      "('reading', 'word')\n",
      "('word', 'write')\n",
      "('write', 'low')\n",
      "('low', 'feel')\n",
      "('feel', 'weight')\n",
      "('weight', 'world')\n",
      "('world', 'bone')\n",
      "('bone', 'try')\n",
      "('try', 'swim')\n",
      "('swim', 'sinking')\n",
      "('sinking', 'alone')\n",
      "('alone', 'always')\n",
      "('always', 'falling')\n",
      "('falling', 'deep')\n",
      "('deep', 'unknown')\n",
      "('unknown', 'fighting')\n",
      "('fighting', 'hand')\n",
      "('hand', 'hand')\n",
      "('hand', 'feel')\n",
      "('feel', 'bullet')\n",
      "('bullet', 'head')\n",
      "('head', 'rush')\n",
      "('rush', 'head')\n",
      "('head', 'rush')\n",
      "('rush', 'see')\n",
      "('see', \"can't\")\n",
      "(\"can't\", 'touch')\n",
      "('touch', \"can't\")\n",
      "(\"can't\", 'touch')\n",
      "('touch', \"'cause\")\n",
      "(\"'cause\", 'feel')\n",
      "('feel', 'numb')\n",
      "('numb', 'infected')\n",
      "('infected', 'bad')\n",
      "('bad', 'blood')\n",
      "('blood', 'bad')\n",
      "('bad', 'blood')\n",
      "('blood', 'keep')\n",
      "('keep', 'running')\n",
      "('running', 'till')\n",
      "('till', 'blow')\n",
      "('blow', 'blow')\n",
      "('blow', 'wanted')\n",
      "('wanted', 'real')\n",
      "('real', 'love')\n",
      "('love', 'feel')\n",
      "('feel', 'numb')\n",
      "('numb', 'silent')\n",
      "('silent', 'voice')\n",
      "('voice', 'distant')\n",
      "('distant', 'crowd')\n",
      "('crowd', 'still')\n",
      "('still', 'singing')\n",
      "('singing', \"there's\")\n",
      "(\"there's\", 'one')\n",
      "('one', 'around')\n",
      "('around', 'keep')\n",
      "('keep', 'screaming')\n",
      "('screaming', 'till')\n",
      "('till', 'lung')\n",
      "('lung', 'run')\n",
      "('run', 'one')\n",
      "('one', 'listens')\n",
      "('listens', 'word')\n",
      "('word', 'coming')\n",
      "('coming', 'low')\n",
      "('low', 'feel')\n",
      "('feel', 'weight')\n",
      "('weight', 'world')\n",
      "('world', 'bone')\n",
      "('bone', 'try')\n",
      "('try', 'swim')\n",
      "('swim', 'sinking')\n",
      "('sinking', 'alone')\n",
      "('alone', 'always')\n",
      "('always', 'falling')\n",
      "('falling', 'deep')\n",
      "('deep', 'unknown')\n",
      "('unknown', 'fighting')\n",
      "('fighting', 'hand')\n",
      "('hand', 'hand')\n",
      "('hand', 'feel')\n",
      "('feel', 'bullet')\n",
      "('bullet', 'head')\n",
      "('head', 'rush')\n",
      "('rush', 'head')\n",
      "('head', 'rush')\n",
      "('rush', 'see')\n",
      "('see', \"can't\")\n",
      "(\"can't\", 'touch')\n",
      "('touch', \"can't\")\n",
      "(\"can't\", 'touch')\n",
      "('touch', \"'cause\")\n",
      "(\"'cause\", 'feel')\n",
      "('feel', 'numb')\n",
      "('numb', 'infected')\n",
      "('infected', 'bad')\n",
      "('bad', 'blood')\n",
      "('blood', 'bad')\n",
      "('bad', 'blood')\n",
      "('blood', 'keep')\n",
      "('keep', 'running')\n",
      "('running', 'till')\n",
      "('till', 'blow')\n",
      "('blow', 'blow')\n",
      "('blow', 'wanted')\n",
      "('wanted', 'real')\n",
      "('real', 'love')\n",
      "('love', 'feel')\n",
      "('feel', 'numb')\n",
      "('numb', 'feel')\n",
      "('feel', 'numb')\n",
      "('numb', 'numb')\n",
      "('numb', 'wanted')\n",
      "('wanted', 'real')\n",
      "('real', 'love')\n",
      "('love', 'numb')\n",
      "('numb', 'feel')\n",
      "('feel', 'numb')\n",
      "('numb', 'feel')\n",
      "('feel', 'numb')\n",
      "('numb', 'numb')\n",
      "('numb', 'wanted')\n",
      "('wanted', 'real')\n",
      "('real', 'love')\n",
      "('love', 'numb')\n",
      "('numb', 'feel')\n",
      "('feel', 'numb')\n",
      "('numb', 'low')\n",
      "('low', 'feel')\n",
      "('feel', 'weight')\n",
      "('weight', 'world')\n",
      "('world', 'bone')\n",
      "('bone', 'try')\n",
      "('try', 'swim')\n",
      "('swim', 'sinking')\n",
      "('sinking', 'alone')\n",
      "('alone', 'always')\n",
      "('always', 'falling')\n",
      "('falling', 'deep')\n",
      "('deep', 'unknown')\n",
      "('unknown', 'fighting')\n",
      "('fighting', 'hand')\n",
      "('hand', 'hand')\n",
      "('hand', 'feel')\n",
      "('feel', 'bullet')\n",
      "('bullet', 'head')\n",
      "('head', 'rush')\n",
      "('rush', 'head')\n",
      "('head', 'rush')\n",
      "('rush', 'see')\n",
      "('see', \"can't\")\n",
      "(\"can't\", 'touch')\n",
      "('touch', \"can't\")\n",
      "(\"can't\", 'touch')\n",
      "('touch', \"'cause\")\n",
      "(\"'cause\", 'feel')\n",
      "('feel', 'numb')\n",
      "('numb', 'infected')\n",
      "('infected', 'bad')\n",
      "('bad', 'blood')\n",
      "('blood', 'bad')\n",
      "('bad', 'blood')\n",
      "('blood', 'keep')\n",
      "('keep', 'running')\n",
      "('running', 'till')\n",
      "('till', 'blow')\n",
      "('blow', 'blow')\n",
      "('blow', 'wanted')\n",
      "('wanted', 'real')\n",
      "('real', 'love')\n",
      "('love', 'feel')\n",
      "('feel', 'numb')\n",
      "('numb', 'feel')\n",
      "('feel', 'numb')\n",
      "('numb', 'numb')\n",
      "('numb', 'wanted')\n",
      "('wanted', 'real')\n",
      "('real', 'love')\n",
      "('love', 'numb')\n",
      "('numb', 'feel')\n",
      "('feel', 'numb')\n",
      "('numb', 'feel')\n",
      "('feel', 'numb')\n",
      "('numb', 'numb')\n",
      "('numb', 'wanted')\n",
      "('wanted', 'real')\n",
      "('real', 'love')\n",
      "('love', 'feel')\n",
      "('feel', 'numb')\n",
      "('numb', \"i've\")\n",
      "(\"i've\", 'never')\n",
      "('never', 'seen')\n",
      "('seen', 'diamond')\n",
      "('diamond', 'flesh')\n",
      "('flesh', 'cut')\n",
      "('cut', 'teeth')\n",
      "('teeth', 'wedding')\n",
      "('wedding', 'ring')\n",
      "('ring', 'movie')\n",
      "('movie', 'proud')\n",
      "('proud', 'address')\n",
      "('address', 'torn')\n",
      "('torn', 'uptown')\n",
      "('uptown', 'post')\n",
      "('post', 'code')\n",
      "('code', 'envy')\n",
      "('envy', 'every')\n",
      "('every', \"song's\")\n",
      "(\"song's\", 'like')\n",
      "('like', 'gold')\n",
      "('gold', 'teeth')\n",
      "('teeth', 'grey')\n",
      "('grey', 'goose')\n",
      "('goose', 'tripping')\n",
      "('tripping', 'bathroom')\n",
      "('bathroom', 'bloodstain')\n",
      "('bloodstain', 'ball')\n",
      "('ball', 'gown')\n",
      "('gown', 'trashing')\n",
      "('trashing', 'hotel')\n",
      "('hotel', 'room')\n",
      "('room', 'care')\n",
      "('care', \"we're\")\n",
      "(\"we're\", 'driving')\n",
      "('driving', 'cadillacs')\n",
      "('cadillacs', 'dream')\n",
      "('dream', \"everybody's\")\n",
      "(\"everybody's\", 'like')\n",
      "('like', 'crystal')\n",
      "('crystal', 'maybach')\n",
      "('maybach', 'diamond')\n",
      "('diamond', 'timepiece')\n",
      "('timepiece', 'jet')\n",
      "('jet', 'plane')\n",
      "('plane', 'island')\n",
      "('island', 'tiger')\n",
      "('tiger', 'gold')\n",
      "('gold', 'leash')\n",
      "('leash', 'care')\n",
      "('care', 'caught')\n",
      "('caught', 'love')\n",
      "('love', 'affair')\n",
      "('affair', \"we'll\")\n",
      "(\"we'll\", 'never')\n",
      "('never', 'royal')\n",
      "('royal', 'run')\n",
      "('run', 'blood')\n",
      "('blood', 'kind')\n",
      "('kind', 'lux')\n",
      "('lux', \"ain't\")\n",
      "(\"ain't\", 'u')\n",
      "('u', 'crave')\n",
      "('crave', 'different')\n",
      "('different', 'kind')\n",
      "('kind', 'buzz')\n",
      "('buzz', 'let')\n",
      "('let', 'ruler')\n",
      "('ruler', 'call')\n",
      "('call', 'queen')\n",
      "('queen', 'bee')\n",
      "('bee', 'baby')\n",
      "('baby', \"i'll\")\n",
      "(\"i'll\", 'rule')\n",
      "('rule', \"i'll\")\n",
      "(\"i'll\", 'rule')\n",
      "('rule', \"i'll\")\n",
      "(\"i'll\", 'rule')\n",
      "('rule', \"i'll\")\n",
      "(\"i'll\", 'rule')\n",
      "('rule', 'let')\n",
      "('let', 'live')\n",
      "('live', 'fantasy')\n",
      "('fantasy', 'friend')\n",
      "('friend', \"we've\")\n",
      "(\"we've\", 'cracked')\n",
      "('cracked', 'code')\n",
      "('code', 'count')\n",
      "('count', 'dollar')\n",
      "('dollar', 'train')\n",
      "('train', 'party')\n",
      "('party', 'everyone')\n",
      "('everyone', 'know')\n",
      "('know', 'u')\n",
      "('u', 'know')\n",
      "('know', \"we're\")\n",
      "(\"we're\", 'fine')\n",
      "('fine', 'come')\n",
      "('come', 'money')\n",
      "('money', 'every')\n",
      "('every', \"song's\")\n",
      "(\"song's\", 'like:')\n",
      "('like:', 'gold')\n",
      "('gold', 'teeth')\n",
      "('teeth', 'grey')\n",
      "('grey', 'goose')\n",
      "('goose', 'tripping')\n",
      "('tripping', 'bathroom')\n",
      "('bathroom', 'bloodstain')\n",
      "('bloodstain', 'ball')\n",
      "('ball', 'gown')\n",
      "('gown', 'trashing')\n",
      "('trashing', 'hotel')\n",
      "('hotel', 'room')\n",
      "('room', 'care')\n",
      "('care', \"we're\")\n",
      "(\"we're\", 'driving')\n",
      "('driving', 'cadillacs')\n",
      "('cadillacs', 'dream')\n",
      "('dream', \"everybody's\")\n",
      "(\"everybody's\", 'like:')\n",
      "('like:', 'crystal')\n",
      "('crystal', 'maybach')\n",
      "('maybach', 'diamond')\n",
      "('diamond', 'timepiece')\n",
      "('timepiece', 'jet')\n",
      "('jet', 'plane')\n",
      "('plane', 'island')\n",
      "('island', 'tiger')\n",
      "('tiger', 'gold')\n",
      "('gold', 'leash')\n",
      "('leash', 'care')\n",
      "('care', 'caught')\n",
      "('caught', 'love')\n",
      "('love', 'affair')\n",
      "('affair', \"we'll\")\n",
      "(\"we'll\", 'never')\n",
      "('never', 'royal')\n",
      "('royal', 'run')\n",
      "('run', 'blood')\n",
      "('blood', 'kind')\n",
      "('kind', 'lux')\n",
      "('lux', \"ain't\")\n",
      "(\"ain't\", 'u')\n",
      "('u', 'crave')\n",
      "('crave', 'different')\n",
      "('different', 'kind')\n",
      "('kind', 'buzz')\n",
      "('buzz', 'let')\n",
      "('let', 'ruler')\n",
      "('ruler', 'ruler')\n",
      "('ruler', 'call')\n",
      "('call', 'queen')\n",
      "('queen', 'bee')\n",
      "('bee', 'baby')\n",
      "('baby', \"i'll\")\n",
      "(\"i'll\", 'rule')\n",
      "('rule', \"i'll\")\n",
      "(\"i'll\", 'rule')\n",
      "('rule', \"i'll\")\n",
      "(\"i'll\", 'rule')\n",
      "('rule', \"i'll\")\n",
      "(\"i'll\", 'rule')\n",
      "('rule', 'let')\n",
      "('let', 'live')\n",
      "('live', 'fantasy')\n",
      "('fantasy', \"we're\")\n",
      "(\"we're\", 'better')\n",
      "('better', \"we've\")\n",
      "(\"we've\", 'ever')\n",
      "('ever', 'dreamed')\n",
      "('dreamed', 'love')\n",
      "('love', 'queen')\n",
      "('queen', 'life')\n",
      "('life', 'great')\n",
      "('great', 'without')\n",
      "('without', 'care')\n",
      "('care', 'caught')\n",
      "('caught', 'love')\n",
      "('love', 'affair')\n",
      "('affair', \"we'll\")\n",
      "(\"we'll\", 'never')\n",
      "('never', 'royal')\n",
      "('royal', 'run')\n",
      "('run', 'blood')\n",
      "('blood', 'kind')\n",
      "('kind', 'lux')\n",
      "('lux', \"ain't\")\n",
      "(\"ain't\", 'u')\n",
      "('u', 'crave')\n",
      "('crave', 'different')\n",
      "('different', 'kind')\n",
      "('kind', 'buzz')\n",
      "('buzz', 'let')\n",
      "('let', 'ruler')\n",
      "('ruler', 'ruler')\n",
      "('ruler', 'call')\n",
      "('call', 'queen')\n",
      "('queen', 'bee')\n",
      "('bee', 'baby')\n",
      "('baby', \"i'll\")\n",
      "(\"i'll\", 'rule')\n",
      "('rule', \"i'll\")\n",
      "(\"i'll\", 'rule')\n",
      "('rule', \"i'll\")\n",
      "(\"i'll\", 'rule')\n",
      "('rule', \"i'll\")\n",
      "(\"i'll\", 'rule')\n",
      "('rule', 'let')\n",
      "('let', 'live')\n",
      "('live', 'fantasy')\n"
     ]
    }
   ],
   "source": [
    "gram_tree = ngrams(dict_new, 2) # биграмма\n",
    "gram_list = []\n",
    "for gram in gram_tree:\n",
    "    gram_list.append(gram)\n",
    "    print(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "3532c42f-1f5b-48f5-8779-edc73ed94228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "553"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gram_list) #количество биграм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "f53d90b2-9fa3-40f0-9ba2-30dc09a69e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(gram_list)) #уникальные биграмы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "f197b8f1-fedd-4eef-b0ad-0abc9398e7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('feel', 'numb'), 14),\n",
       " ((\"i'll\", 'rule'), 12),\n",
       " (('rule', \"i'll\"), 9),\n",
       " (('fly', 'high'), 8),\n",
       " (('wanted', 'real'), 7),\n",
       " (('real', 'love'), 7),\n",
       " (('numb', 'feel'), 7),\n",
       " (('head', 'rush'), 6),\n",
       " ((\"can't\", 'touch'), 6),\n",
       " (('bad', 'blood'), 6)]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#проверим самые частые словосочетания\n",
    "counts = Counter(gram_list)\n",
    "counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b693446-f9b6-4d24-8943-b81ca87215ec",
   "metadata": {},
   "source": [
    "### На этом все) Спасибо за внимание!\n",
    "\n",
    "<a href=\"https://ibb.org.ru/1/Ik7HWD\"><img src=\"https://ibb.org.ru/images/2024/05/23/8c4a51e005629a084505649079b0a949.jpg\" alt=\"8c4a51e005629a084505649079b0a949.jpg\" border=\"0\"></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
